{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/PyTorch101_ODSC_Europe2020/master/images/linear_dogs.jpg\" height=\"400\" width=\"800\"> \n",
    "\n",
    "# Глубокое обучение и вообще: домашнее задание 1\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## О задании\n",
    "\n",
    "Это задание будет состоять из нескольких частей.\n",
    "\n",
    "- __[2 балла]__ В первой части вы попробуете попрактиковаться в работе с тензорами.\n",
    "- __[2.5 балла]__ Во второй части вы напишите на pytorch свою собственную логистическую регрессию. \n",
    "- __[5.5 балла]__ В третьей части вы немного поупражняетесь с нейросетками на табличных встроенных в pytorch данных.\n",
    "- __[2 балла]__ Четвёртая часть будет бонусной. В ней вам будет предложено реализовать несколько различных вариаций градиентного спуска. \n",
    "\n",
    "Баллы даются за выполнение отдельных пунктов. Задачи в рамках одного раздела рекомендуется решать в том порядке, в котором они даны в задании. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 12 баллов. За каждый день просрочки после мягкого дедлайна снимается 1 балл. После жёсткого дедлайна работы не принимаются. Даже при опозданиии на одну секунду. Сдавайте работы заранее. Мягкий дедлайн можно отодвинуть, воспользовавшись **late days policy** \n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Любой из студентов может быть вызван на защиту любого домашнего задания. В таком случае итоговая оценка студента определяется в результате защиты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формат сдачи\n",
    "\n",
    "\n",
    "В форму необходимо загрузить ноутбук с выполенным заданием. Сам ноутбук называйте в формате hw-01-USERNAME.ipynb, где USERNAME — ваши фамилия и имя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оформление\n",
    "\n",
    "1. Be good, drink milk and [think of Russia](https://www.youtube.com/watch?v=jyxSFfBKMxQ)\n",
    "2. Обязательно фиксируйте зерно генератора случайных чисел в экспериментах. При перезапуске кода значения не должны меняться.\n",
    "3. Вверху файла подпишите фамилию, имя и какой-то занимательный факт о себе.\n",
    "4. Обратите внимание, что у графиков должны быть подписаны оси, заголовок графика и при необходимости обязательно наличие легенды. \n",
    "\n",
    "> За отсутствие названий графиков и подписей к осям могут снижаться баллы. Все картинки должны быть самодостаточны и визуально удобны для восприятия, так чтобы не нужно было смотреть ваш код или знать задание, чтобы понять что на них изображено.\n",
    "\n",
    "\n",
    "Для каждого построенного графика делайте выводы. Эти выводы не должны быть поверхностными и очевидными. Не будьте мудрым королём.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/andan_2023/main/hw/king.png\" width=\"300\"> \n",
    "</center>\n",
    "\n",
    "**Пример плохого вывода:** Синенькая линия идет вверх, а красная вниз. Черненькая идет вниз, а потом вверх. \n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/andan_2023/main/hw/bad_lines.png\" width=\"600\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Простейшие операции на pytorch\n",
    "\n",
    "В этой части вам надо будет реализовать несколько функций активации. Вы можете пока не знать, какие у них есть свойства и чем они хороши. Разговор об этом пойдёт на будущих лекциях. Достаточно просто познакомиться с ними, вбив формулы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Cоздайте два случайных тензора (двумерных, не квадратных). Умножьте их друг на друга, результат запишите в третий тензор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(6, 8)\n",
    "y = torch.rand(6, 8)\n",
    "\n",
    "z = x * y \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Реализуйте [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) используя только pytorch, примените его к тензору x (запрещено использование модулей torch.nn и его подмодулей, а также torch.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    return torch.maximum(torch.zeros_like(x), x)\n",
    "\n",
    "assert torch.all(F.relu(x) == relu_forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Сделайте тоже самое c ELU, [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формулу ELU взял на вики, где и ReLU\n",
    "# делаю тензор из 1 и умножаю потом на константу(только по формуле она -const) \n",
    "# получаю тензор из моих -const и сравниваю через torch\n",
    "\n",
    "def elu_forward(x):\n",
    "    return torch.maximum(torch.ones_like(x) * -1e-4 , x)\n",
    "\n",
    "assert torch.allclose(F.elu(x), elu_forward(x), 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Сделайте тоже самое c LeakyReLU, [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формулу  LeakyReLU взял на вики, где и ReLU\n",
    "def lrelu_forward(x, alpha):\n",
    "    return torch.maximum(alpha * x, x)\n",
    "\n",
    "assert torch.all(F.leaky_relu(x, 0.01) == lrelu_forward(x, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Теперь перейдем к немного более современным функциям активаций, например Mish, она выглядит следующим образом:\n",
    "\n",
    "$$x \\cdot tanh(ln(1+e^x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(inputs):\n",
    "    return(inputs * torch.tanh(torch.log(1 + torch.exp(inputs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    mish(torch.tensor([1, 1, 1], dtype=torch.float32)),\n",
    "    torch.tensor([0.8651, 0.8651, 0.8651]),\n",
    "    atol=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    mish(torch.tensor([0.6376, 0.4021, 0.6656, 0.3726], dtype=torch.float64)),\n",
    "    torch.tensor([0.5014, 0.2908, 0.5280, 0.2663], dtype=torch.float64),\n",
    "    atol=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Теперь реализуем swish, она выглядит следующим образом:\n",
    "\n",
    "$$x \\cdot \\sigma(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * (1/(1+torch.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    swish(torch.tensor([1, 1, 1], dtype=torch.float32)),\n",
    "    torch.tensor([0.7311, 0.7311, 0.7311]),\n",
    "    atol=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    swish(torch.tensor([0.6376, 0.4021, 0.6656, 0.3726], dtype=torch.float64)),\n",
    "    torch.tensor([0.4171, 0.2409, 0.4396, 0.2206], dtype=torch.float64),\n",
    "    atol=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __[0.5]__ Напишите на pytorch код, который решит следующую задачу оптимизации с помощью градиентного спуска:\n",
    "\n",
    "$$\n",
    "f(x) = x^2 - 2x + 5 \\to \\min_{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем необходимые для нашего решения переменные\n",
    "x = torch.tensor([2.5], requires_grad=True) \n",
    "print(x)\n",
    "alpha = 0.1\n",
    "epochs = 77 # можно взять и меньше для нашей задачи \n",
    "\n",
    "# задаем функцию, которая будет принимать на вход Х и делать манипулцию в функцию\n",
    "def function(x):\n",
    "    return x**2 - 2*x + 5\n",
    "#########\n",
    "for epoch in range(epochs):\n",
    "    y = function(x) \n",
    "    #print(y) #там уже 4 устойчиво появляется, поэтому как говорил эпохи можно уменьшить, но оставлю 77 -> nice\n",
    "    y.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= alpha * x.grad \n",
    "    x.grad.zero_() #зануляем градиент, чтобы они не вхлодили в мат. операции, как было сказано у тебя в коде)\n",
    "print('_____________________')    \n",
    "print('Output:', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Логистическая регрессия\n",
    "\n",
    "В этом задании мы соберём на PyTorch логистическую регрессию Для её обучения будем использовать игрушечные данные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "(X, y) = datasets.make_circles(n_samples=1024, shuffle=True, \n",
    "                               noise=0.2, factor=0.4, random_state=42)\n",
    "\n",
    "ind = np.logical_or(y == 1, X[:, 1] > X[:, 0] - 0.5)\n",
    "X = X[ind, :]\n",
    "m = np.array([[1, 1], [-2, 1]])\n",
    "X = preprocessing.scale(X)\n",
    "y = y[ind]\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Рзбейте выборку на обучающую, валидационную и тестовую. Для обучающей и валидационной выборок заведите Dataset и DataLoader: \n",
    "\n",
    "- для обучающей: итератор должен перемешивать выборку каждую эпоху и генерировать батчи размера $32$\n",
    "- для валидационной: итератор должен генировать батчи размера $32,$ данные перемешивать не надо\n",
    "- про тестовую выборку пока что забудьте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "Xx_train, Xx_test, yy_train ,yy_test = train_test_split(X , y , test_size = .3 , random_state=42 )\n",
    "\n",
    "X_val, X_testt, y_val, y_testt = train_test_split(Xx_train, yy_train, test_size = .3 , random_state=42 ) \n",
    "\n",
    "print('Arrays')\n",
    "print(\"X_train shape: {}\".format(Xx_train.shape))\n",
    "print(\"X_test shape: {}\".format(Xx_test.shape))\n",
    "print(\"y_train shape: {}\".format(yy_train.shape))\n",
    "print(\"y_test shape: {}\".format(yy_test.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "#https://colab.research.google.com/github/iu5git/MPPR/blob/main/notebooks/Lab3.ipynb#scrollTo=a77Fex1TIhGE\n",
    "# тут можно найти как делать в dataset and dataloader,но можно и ручками быстрее сделать самостоятельно\n",
    "# без циклов и тд так как там просто переводит в тензор\n",
    "\n",
    "X_train = torch.tensor(Xx_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(yy_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(Xx_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(yy_test, dtype=torch.float32)\n",
    "\n",
    "print('Tensors')\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))\n",
    "\n",
    "# присвоение в даталоадер можно глянуть с 1 пары, по аналогии сделаю и датасет\n",
    "# также можно посмотреть по ссылку как делается создание TensorDataset и сделать аналогично)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "validation_dataset =  TensorDataset(X_val, y_val)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32) # shuffle = False по дефолту\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ реализуйте на PyTorch функцию для поиска вероятности того, что объект относится к первому классу.\n",
    "\n",
    "\n",
    "$$ \\hat{y} = \\langle w, x \\rangle $$\n",
    "$$ P( y=1 \\; \\big| \\; x, \\, w) = \\dfrac{1}{1 + \\exp(- \\langle w, x \\rangle)} = \\sigma(\\langle w, x \\rangle)$$\n",
    "\n",
    "Обратите внимание, что по весам мы будем искать градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([1.0,1.0,1.0], requires_grad=True) \n",
    "\n",
    "# можно взять с swish функцию сигмоиду\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def probability(X):\n",
    "    X = torch.cat([X, torch.ones((X.shape[0], 1))], dim=1).float()\n",
    "    y = X @ w\n",
    "    return sigmoid(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для логистической регрессии оптимальный параметр находится минимизацией кросс-энтропии (или logloss): \n",
    "\n",
    "$$ L(w) =  - {1 \\over n} \\sum_{i=1}^n \\left[ {y_i \\cdot log P(y_i = 1 \\, | \\, x_i,w) + (1-y_i) \\cdot log (1-P(y_i = 1 \\, | \\, x_i,w))}\\right] $$\n",
    "\n",
    "\n",
    "__[0.25 балла]__ реализуйте в PyTorch эту функцию потерь. Пользоваться готовыми функциями потерь вроде `CrossEntropyLoss()` нельзя. Надо вбить самостоятельно. Для значений $y$, для удобства, можно сделать One Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y):\n",
    "    result = - torch.mean(y * torch.log(probability(X)) + (1-y) * torch.log(1-probability(X)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.5 балла]__ Напишите функцию, которая делает один шаг обучения логистической регрессии. Для градиентного спуска используйте в качестве оптимизатора `SGD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([w], lr = 0.1)\n",
    "\n",
    "def train_step(X, y):\n",
    "    optimizer.zero_grad()\n",
    "    result = compute_loss(X, y)\n",
    "    result.backward()\n",
    "    optimizer.step()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__ Напишите цикл для обучения модели. Цикл должен делать $1000$ итераций. В случае, если ошибка на валидационной выборке начала расти и растёт в течение $20$ эпох, обучение должно останавливаться. Такая стратегия обучения называется ранней остановкой (early stopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем эпохи, оптимайзер и  индикатор для лоссов, чтобы сравнивать, когда начинает расти в течение 20 эпох\n",
    "# также нужно будет задать флаг, который будет собирать в себе информацию о росте лосса\n",
    "EPOCH = 10000\n",
    "optimizer = torch.optim.SGD([w], lr = 0.1)\n",
    "validation_loss = 9999999\n",
    "flag = 0\n",
    "w = torch.tensor([1.0,1.0,1.0], requires_grad=True) \n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_on_train = 0.0\n",
    "    for marker,target in train_dataloader:\n",
    "        train_step(X_train, y_train) \n",
    "\n",
    "    loss_on_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for marker,target in validation_dataloader:\n",
    "            loss_on_val += compute_loss(marker,target) \n",
    "        loss_on_val /= len(validation_dataloader.dataset)\n",
    "    \n",
    "    if loss_on_val < validation_loss:\n",
    "        validation_loss = loss_on_val\n",
    "        flag = 0\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "    if flag >= 20:\n",
    "        print(\"Ошибка на валид выборке растет уже 20 и более раз, поэтому сделаем раннюю остановку\")\n",
    "        print('---------------')\n",
    "        print(epoch)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Измерите качество итоговой модели на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loss(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Собираем нейросеть\n",
    "\n",
    "В этом задании мы попробуем обучить нейросеть предсказывать цены на недвижимость в Бостоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Отнормируйте данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Найдите медианную цену на обучающей выборке. Используйте её в качестве прогноза. Какое качество в терминах метрики $MAPE$ получается на тестовой выборке? Для всех дальнейших вопросах о качестве используйте эту метрику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Медианное значение цены на обучающей выборке :',np.median(y_train))\n",
    "med_price = np.median(y_train) \n",
    "y_pred = med_price.repeat(len(y_test))\n",
    "print('Метрика МАРЕ :',mean_absolute_percentage_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.5 балла]__ Подгрузите из sklearn модель линейной регрессии и модель случайного леса. Обучите их на наших данных. Какое качество у них получилось на тестовой выборке? Какой функционал они используют для обучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_norm,y_train)\n",
    "y_pred_lr = linreg.predict(X_test_norm)\n",
    "print('Метрика МАРЕ LinReg :',mean_absolute_percentage_error(y_test,y_pred_lr))\n",
    "\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest.fit(X_train_norm,y_train)\n",
    "y_pred_for = forest.predict(X_test_norm)\n",
    "print('Метрика МАРЕ Forest :',mean_absolute_percentage_error(y_test,y_pred_for))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.5 балла]__ Менеджер Николай обладает гуманитарным образованием. Он спрашивает вас, насколько хорошими получились модели. Объясните ему это так, чтобы он понял.\n",
    "\n",
    "__Ответ:__  Если говорить про локальный случай, то можно сказать, что модель случайного леса оказалась лучше, поскольку дает нам меньшую ошибку на этих данных  ≈ 15%,а у лог регрессии ≈ 17%, то есть грубо говоря разница в 2%.С помощью модели случайного леса при предсказании результата мы ошибаемся в 15 случаях из 100, например. В модели лог регрессии в 17 из 100. Много или мало? Ответ будет таким: всегда лучше стараться уменьшить ошибки в модели, поскольку это даст нам лучшие результаты предсказания(\"более точные\").\n",
    "Если выйти за рамки рассматриваемых моделей, то получившиеся модели нельзя назвать \"хорошими\",поскольку ошибки довольно большие и их можно уменьшить с помощью подбора параметров и необходимого препроцессинга. Наши вычисления можно назвать \"сырыми\" и не опираться на них, как на истину, и не рассматривать данные модели как конечный инструмент для предскзаания данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.5 балла]__ Реализуйте линейную регрессию в виде нейронной сети. Для этого используйте один полносвязный слой без функции активации.\n",
    "\n",
    "Обучите её, использая Adam в качестве оптимизатора и $MSE$ в качестве функции потерь. Сравните получившиеся значения коэффициентов с тем, что получилось при обучении модели средствами sklearn. Сравнение делаете с помощью какого-нибудь графика. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train_norm, y_train, \n",
    "                                                              test_size = .3 , random_state=42) \n",
    "\n",
    "X_train = torch.tensor(X_train_split, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_split, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset =  TensorDataset(X_val, y_val)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, train_accuracies, test_accuracies):\n",
    "    clear_output()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(13, 4))\n",
    "    axs[0].plot(range(1, len(train_losses) + 1), train_losses, label='train')\n",
    "    axs[0].plot(range(1, len(test_losses) + 1), test_losses, label='test')\n",
    "    axs[0].set_ylabel('loss')\n",
    "\n",
    "    axs[1].plot(range(1, len(train_accuracies) + 1), train_accuracies, label='train')\n",
    "    axs[1].plot(range(1, len(test_accuracies) + 1), test_accuracies, label='test')\n",
    "    axs[1].set_ylabel('accuracy')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear(nn.Module):\n",
    "    def __init__(self,in_features):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features = in_features, out_features= 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device  = torch.device('cpu:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = linear(32,13)\n",
    "optimizer_adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion_MSE = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_load, val_load):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_load:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_load:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, criterion_MSE, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.5 балла]__ Реализуйте две разных архитектуры нейросетей, которые принципиально отличаются друг от друга. Обучите каждую из них с двумя оптимизаторами: Adam и SGD. Визуализируйте все траектории обучения на картинке. \n",
    "\n",
    "Не забывайте отщипнуть в рамках этого метода от обучающей выборки кусочек для валидации. Тестовую выборку мы будем использовать только для итоговой оценки качества. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class drop(nn.Module):\n",
    "    def __init__ (self, in_features, hidden_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size//4, hidden_size//8),\n",
    "            nn.ReLU,\n",
    "            nn.Linear(hidden_size//8, 1))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features, hide_neurons):\n",
    "        super(base, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features, hide_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hide_neurons, hide_neurons // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hide_neurons//4, hide_neurons//8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hide_neurons//8, 1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = base(32,13).to(device)\n",
    "model_sgd = base(15,25).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=0.01)\n",
    "criterion_a = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_adam, optimizer_adam, criterion, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here  (ﾉ◕ヮ◕)ﾉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__ Потратьте некоторое время на модернизацию своих архитектур. Попытайтесь побить качество, которое дал случайный лес. Опишите здесь свои эксперименты что/как/почему вы пробовали. Оставьте в виде кода реализацию лучшей модели. \n",
    "\n",
    "__Добавлять сюда описание своих экспериментов в виде текста - обязательно! Без этого полного балла не будет.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here  ٩(⁎❛ᴗ❛⁎)۶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.5 балла]__ Возьмите лучшую из получившихся у вас моделей. Обучите её на неотнормированных данных и на отнормированных данных. Визуализируйте траектории обучения обеих моделей на одной картинке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here  (︺︹︺)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Какое поведение в этих траекториях вы ожидали увидеть? Какое поведение вы увидели в реальности? Как думаете, почему траектории ведут себя именно так? При обучении моделей выше мы использовали $MSE$. Какой есть минус у этого функционала? \n",
    "\n",
    "__Ответ:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[1 балл]__  Давайте попробуем обучить ту же самую модель, используя в качестве функций потерь другие функционалы: \n",
    "\n",
    "- MAE\n",
    "- MSLE\n",
    "- LogCosh \n",
    "\n",
    "Каким оказывается MAPE на тестовой выборке для всех трёх случаев?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here  ♪┏(・o･)┛♪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.25 балла]__ Какой смысл кроется за этими функциями потерь? Как думаете, какая из них лучше всего подходит для решения нашей задачи? Объясните почему. \n",
    "\n",
    "В этом вам поможет [глава из книги Александра Дьяконова](https://alexanderdyakonov.files.wordpress.com/2018/10/book_08_metrics_12_blog1.pdf) и [конспект из курса МО-1 Жени Соколова](https://github.com/esokolov/ml-course-hse/blob/master/2021-fall/lecture-notes/lecture02-linregr.pdf)\n",
    "\n",
    "__Ответ:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Нейронные сети не очень хорошо показывают себя на табличных данных из-за их разнородности. С такими задачами обычно хорошо справляется градиентный бустинг. Нейронные сети больше подходят для работы с картинками и текстами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Бонус (0.1 балла)]**  Прикрепите фотографию того, как вы начали этот февраль. Какую самую классную эмоцию вы испытали за прошедший месяц?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Бонусный трэк: градиентные спуски\n",
    "\n",
    "В этом разделе вы можете попробовать самостоятельно реализовать разные виды градиентных спусков для логистической регрессии. В PyTorch многие оптимизаторы уже реализованы и нам остаётся их только использовать. Тут мы реализуем все эти оптимизаторы с нуля. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "(X, y) = datasets.make_circles(n_samples=1024, shuffle=True, \n",
    "                               noise=0.2, factor=0.4, random_state=42)\n",
    "\n",
    "ind = np.logical_or(y == 1, X[:, 1] > X[:, 0] - 0.5)\n",
    "X = X[ind, :]\n",
    "m = np.array([[1, 1], [-2, 1]])\n",
    "X = preprocessing.scale(X)\n",
    "y = y[ind]\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим вектор из единичек\n",
    "X = np.hstack((X, np.ones(X.shape[0])[:,np.newaxis]))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.2 балла]__ реализуйте в `numpy` функцию для поиска вероятности того, что объект относится к первому классу\n",
    "\n",
    "\n",
    "$$ \\hat{y} = \\langle w, x \\rangle $$\n",
    "$$ P( y=1 \\; \\big| \\; x, \\, w) = \\dfrac{1}{1 + \\exp(- \\langle w, x \\rangle)} = \\sigma(\\langle w, x \\rangle)$$\n",
    "\n",
    "Обратите внимание, что веса теперь идут на вход в эту функцию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно также взять функцию сигмоиду, сделать ее в нумпай и написать для перемножения массивов\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def probability(X, w):\n",
    "    return sigmoid(np.dot(X,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2], [3,4]])\n",
    "y = np.array([1, 0])\n",
    "w = np.array([0.5, 0.5])\n",
    "\n",
    "assert np.allclose(probability(X, w), np.array([0.81757448, 0.97068777]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.2 балла]__ По аналогии реализуйте логистические потери. \n",
    "\n",
    "$$ L(w) =  - {1 \\over n} \\sum_{i=1}^n \\left[ {y_i \\cdot log P(y_i = 1 \\, | \\, x_i,w) + (1-y_i) \\cdot log (1-P(y_i = 1 \\, | \\, x_i,w))}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w):\n",
    "    result = -1/len(y) * np.sum((y * np.log(probability(X, w)) + (1-y) * np.log(1-probability(X, w))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert compute_loss(X, y, w) - 1.865581 < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[0.4 балла]__ Мы будем обучать модель методом градиентного спуска своими руками. Для этого нам придётся вычислить градиент функции потерь, представленной выше. Возьмите листочек, ручку и в бой! \n",
    "\n",
    "$$ \\nabla_w L = ...$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(X, y, w):\n",
    "    # Рассчитываем количество данных\n",
    "    n = y.shape[0]\n",
    "    # Вычисляем вероятности класса 1 для каждого примера\n",
    "    t = np.dot(X, w)\n",
    "    sigm = sigmoid(t)\n",
    "    # Вычисляем градиент функции потерь кросс-энтропии\n",
    "    grad = np.dot(X.T, (sigm - y)) / n\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(compute_grad(X, y, w),np.array([1.36481889, 1.75895001]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ниже предназначена для визуализации процесса обучения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "def visualize(X, y, w, history):\n",
    "    \"\"\"С помощью магии matplolib выдаёт красоты результатов классификации\"\"\"\n",
    "    Z = probability(np.c_[xx.ravel(), yy.ravel()], w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history)\n",
    "    plt.grid()\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.ylim(0, ymax)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_weights = np.linspace(-1, 1, 2)\n",
    "visualize(X, y, dummy_weights, [2, 0.5, .25, .1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пришло время обучить нашу модель. Для этого вам придётся дописать кусочки функций ниже. Обязательно попробуйте поменять гиперпараметры (размер батча и скорость обучения) и посмотреть как будет изменяться анимация. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.4  балла] Mini-batch SGD\n",
    "\n",
    "Берём несколько рандомных наблюдений и ищем градиент по ним! \n",
    "\n",
    "$$ w_t = w_{t-1} - \\eta \\dfrac{1}{m} \\sum_{j=1}^m \\nabla_w L(w_t, x_{i_j}, y_{i_j}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "w = np.array([0, 1.]) # веса\n",
    "\n",
    "eta= 0.1 \n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12, 5))\n",
    "history = []\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(0, X.shape[0], batch_size):\n",
    "        x_batch = X[j: j + batch_size]\n",
    "        #print(f'Иксы:{x_batch}')\n",
    "        #print(f'Веса:{w}')\n",
    "        y_batch = y[j: j + batch_size]\n",
    "        gradient = compute_grad(x_batch, y_batch, w)\n",
    "        w -= eta * gradient / batch_size\n",
    "    loss = compute_loss(x_batch, y_batch, w)\n",
    "    history.append(loss)\n",
    "visualize(X, y, w, history)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0, 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.4 балла] Momentum SGD\n",
    "\n",
    "Momentum это метод, который помогает стохастическому градиентному спуску сохранять направление движения. Это осуществляется за счёт добавления в выражение дополнительного слагаемого: накопленного за предыдущие шаги градиента с весом $\\alpha$. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ \\nu_t = \\alpha \\nu_{t-1} + \\eta\\dfrac{1}{m} \\sum_{j=1}^m \\nabla_w L(w_t, x_{i_j}, y_{i_j}) $$\n",
    "$$ w_t = w_{t-1} - \\nu_t$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "w = np.array([0, 1.0])\n",
    "\n",
    "eta = 0.05 \n",
    "alpha = 0.2\n",
    "nu = np.zeros_like(w)\n",
    "\n",
    "history = []\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(0, X.shape[0], batch_size):\n",
    "        x_batch = X[j: j + batch_size]\n",
    "        y_batch = y[j: j + batch_size]\n",
    "        gradient = compute_grad(x_batch, y_batch, w)\n",
    "        nu = alpha * nu + eta * gradient / batch_size\n",
    "        w -= nu\n",
    "    loss = compute_loss(X, y, w)\n",
    "    history.append(loss)\n",
    "\n",
    "visualize(X, y, w, history)\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.4 балла] RMSprop\n",
    "\n",
    "В этом блоке реализуем RMSprop. Эта вариация градиентного спуска позволяет изменять скорость обучения индивидуально для каждого параметра. \n",
    "\n",
    "$$ G_t^j = \\alpha G_{t-1}^j + (1 - \\alpha) g_{tj}^2 $$\n",
    "$$ w_t^j = w_{t-1}^j - \\dfrac{\\eta}{\\sqrt{G_t^j + \\varepsilon}} g_{tj} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "w = np.array([0, 1.]) \n",
    "\n",
    "eta = 0.1 \n",
    "alpha = 0.9 \n",
    "g2 = np.zeros_like(w)\n",
    "eps = 1e-8\n",
    "\n",
    "history = []\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "for i in range(n_iter):\n",
    "    for j in range(0, X.shape[0], batch_size):\n",
    "        X_batch = X[j: j + batch_size]\n",
    "        y_batch = y[j: j + batch_size]\n",
    "        gradient = compute_grad(x_batch, y_batch, w)\n",
    "        g2 = alpha * g2 + (1 - alpha) * gradient ** 2\n",
    "        w -=  eta / np.sqrt(g2 + eps) * gradient\n",
    "\n",
    "    loss = compute_loss(X, y, w)\n",
    "    history.append(loss)\n",
    "\n",
    "\n",
    "visualize(X, y, w, history)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как траектории обучения различных вариаций градиентного спуска различаются между собой? Ожидаемо ли это? Почему? Что нужно сделать, чтобы реализовать Adam? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Траектории обучения различных вариаций градиентного спуска  различаются в зависимости от выбранного метода оптимизации и параметров обучения. Например, первые два варианта сходятся медленнее, чем метод, который использует более сложные шаги обновления, такой как  RMSProp.\n",
    "Это ожидаемо, так как разные методы оптимизации имеют разные способы адаптации шага обновления к изменению градиента. Некоторые методы могут быть более эффективными для определенных задач, в то время как другие методы могут показать лучшие результаты для других задач.**\n",
    "\n",
    "\n",
    "**Чтобы реализовать Adam, необходимо выполнить следующие шаги:**\n",
    "- : 1 Инициализировать переменные. Например, скорость обновления и скользящее среднее квадратов градиента.\n",
    "- : 2 Вычислить градиент функции потерь по всем параметрам модели.\n",
    "- : 3 Обновить скользящее среднее квадратов градиента и скорость обновления.\n",
    "- : 4 Вычислить скорректированные значения скользящих средних и скорости обновления.\n",
    "- : 5 Обновить параметры модели с использованием скорректированных значений.\n",
    "- : 6 Повторять шаги 2-5 для каждой эпохи обучения.\n",
    "\n",
    "**Важно отметить, что параметры Adam могут зависеть от конкретной задачи и могут потребовать настройки для достижения лучших результатов.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
